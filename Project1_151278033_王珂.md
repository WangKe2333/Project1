---
title: "Project1 Mapreduce基础编程"
author: "wangke"
date: "2017年11月21日"
output:
  html_document: default
  pdf_document: default
---
#一.实验设计说明
<1>主要设计思路</br>
首先，针对需求1，第一，要从文件中提取出新闻标题；第二，找到Java中文分词的方式，寻找网上开源的代码和中文分词器，而后对新闻标题进行中文分词；第三，对分词后的文件进行停词，忽略一些不必要的停词；第四，在Wordcount的基础上进行改变，统计新闻分词的词频，根据用户输入的k,输出k以上频率的词组；第五，新建一个sort job，以前四步的输出作为新job的输入；第六，调换key-value的顺序，利用系统自带的排序算法进行排序并将词频按照从高到低的顺序输出。</br>
而后，针对需求2，第一，也是将新闻标题进行中文分词，key是新闻标题的分词结果，value使用url+词频；第二，修改wordcount程序，将mapper,combiner,reducer的输入输出类型都设为Text,以便后续处理；第三，将url值与新闻标题词汇共同作为key，value值仍为1；第四，在combiner当中将key还原为新闻标题词汇，value为url+词频；第五，在reducer当中形成url列表并输出。</br>
<2>算法设计</br>
1.Java中文分词：对于中文分词部分，我首先选用了网上的word朴素最大正向匹配算法的程序进行分词，能够达到正常的分词效果，但是后来发现这个程序运行太慢（运行完全部fulldata程序需要大概300分钟），鉴于这个情况，我选择换一个分词器进行分词，来提高程序运行效率，以便于后续程序运行调试等。</br>
2.Java中文分词改进：后来更换了很多同学都使用的IkAnaylazer分词器，速度果然大有提升。在程序中间套用IKAnalyazer 的demo的例子即可</br>
3.按照value值排序输出：建立一个新的Job,上一个程序的输出等于此次的输入。使用InverseMapper改变key和value的位置，编写新的按照降序输出的程序</br>
4.停词：最初使用wordcount2的skip功能忽略部分停词，skip功能对于忽略词语输入的格式有要求，需要在要忽略的词前添加\，故在读入停词文件之前，我又对停词文件的格式做了一定的处理。</br>
5.停词：后来发现IK Analyzer自带停词功能，只需要将配置文件写好放入src中即可，但是一开始并无效果，后发现原来文件需要放错了位置，相应的jar文件也必须引入。
6.倒排索引：为了方便输出和传值，我们将mapper combiner reducer中的输入输出全部换为text类型，在mapper中使用IK Analazer分词，分词后，key值为新闻标题+url值，value值仍为1；在combiner中用value统计词频，设置value值为url+词频，设置key值为新闻词汇；在reducer中将url合并为urllist并输出</br>
<3>主要程序代码</br>
分词程序</br>
```{}
//创建分词对象  
Analyzer anal=new IKAnalyzer(true);       
StringReader reader=new StringReader(itr.nextToken());
//分词  
TokenStream ts=anal.tokenStream("", reader);  
CharTermAttribute term=ts.getAttribute(CharTermAttribute.class);  
while(ts.incrementToken()){  
      word.set(term.toString());
      context.write(word, one);
}
```
提取新闻标题</br>
```{}
String[] strarray=line.split("\t"); 
     if(strarray.length>5)
     {
    	 line=strarray[4];
     }
```
接受用户输入k</br>
```{}
String k=context.getConfiguration().get("k");
int t=Integer.parseInt(k);
if(result.get()>t)
{
    context.write(key, result);
}
```

文档倒排索引</br>
```{}
//mapper部分
word.set(term.toString()+":"+line2);
word2.set("1");
context.write(word, word2);
//combiner部分
info.set( key.toString().substring( splitIndex + 1) +":"+sum );
key.set( key.toString().substring(0,splitIndex));
//reducer部分
for (Text value : values) {
    urlList += value.toString()+";";
}
    result.set(urlList);
```
<4>程序类的说明
1.project.java文件中</br>
mapper类：可以做到wordcount的skip-patterns和提取新闻标题，中文分词，停词</br>
reducer类：累加统计词频，将大于k的部分输出</br>
comparator类：实现降序输出</br>
main:接受用户输入k，一次建立wordcount和sort两个job，前一个job的输出正好是后一个job的输入</br>
2.InvertedIndex--wordco.java文件中</br>
mapper类：实现分词，key为新闻词汇+url，value值为1</br>
combiner类：统计词频，设置value为url+词频，key为新闻词汇</br>
reducer类：生成url列表</br>

#程序运行结果说明和分析
1.利用wordseg朴素最大正向匹配算法分词，程序运行非常慢</br>
![程序很慢](https://github.com/WangKe2333/Project1/raw/master/picture/wordseg运行很慢.png)
</br>但在小数据集上可以正常做到分词</br>
![wordseg分词](https://github.com/WangKe2333/Project1/raw/master/picture/wordseg_小数据集.png)
</br>2.利用IKAnalyzer分词结果：做到了合理的分词，停词和排序</br>
![IKAnalyzer分词](https://github.com/WangKe2333/Project1/raw/master/picture/分词结果.png)
</br>设置k值为100：可以看到词频为100以上的才被输出</br>
![k=100](https://github.com/WangKe2333/Project1/raw/master/picture/k%3D100.png)
</br>3.文档倒排索引：准确输出了词汇，对应的url和词频</br>
![文档倒排](https://github.com/WangKe2333/Project1/raw/master/picture/文档倒排.png)
![文档倒排2](https://github.com/WangKe2333/Project1/raw/master/picture/文档倒排2.png)

#project1过程中遇到的问题及解决
1.最让人崩溃的bug:有的类运行太多次之后，再导出新的jar包运行mapreduce仍然在运行之前的jar包和类，新的修改并不起作用。改啊改啊改了一整天怎么改输出都不变，崩溃一天之后才发现MapReduce一直在运行一个jar包，修改类名，新建project，代码不做修改即可解决。</br>
2.传入参数一直未null值：配置参数一定要在新建job之前，否则无法传入参数</br>
3.IKAnalyzer包运行，要打成runnable jar file,配置文件和停词文件也要一同加入</br>
4.停词文件要求需要为UTF-8格式，可以直接用vim查看和修改:set fileencoding=</br>
5.文档倒排索引最好将mapper reducer的输入输出设置为text格式，统一比较容易处理</br>
6.大文件当中有一行为空，做分词等处理的时候出现问题</br>


#性能扩展性不足及可能的改进之处
1.程序功能较为单一，在分词方面直接调用了分词器和开源程序的内容，没有做更加细致的处理</br>
2.停词能够支持的文件只能是已经设置好的，用户想自定义停词程序可以使用wordcount的skip功能实现</br>
3.文档倒排索引部分可以输出更多的信息，将该词汇所在的文件输出，但是这样一来程序输出的东西太多，不太直观，后续可以考虑让用户通过输入设置选择按照何种方式倒排索引</br>

</br>
</br>
</br>










